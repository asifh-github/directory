<html><div class="jobsearch-JobInfoHeader-title-container">
<h1 class="icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title">
              Data Engineer
             </h1>
</div>None<div class="jobsearch-JobDescriptionSection-section" id="jobDetailsSection">
<div class="jobsearch-JobDescriptionSection-title">
<h2 class="jobsearch-JobDescriptionSection-title--main icl-u-textBold" id="jobDetails" tabindex="-1">
               Job details
              </h2>
</div>
<div class="jobsearch-JobDescriptionSection-sectionItem">
<div class="jobsearch-JobDescriptionSection-sectionItemKey icl-u-textBold">
               Salary
              </div>
<span class="icl-u-xs-mr--xs">
               $73 an hour
              </span>
</div>
<div class="jobsearch-JobDescriptionSection-sectionItem">
<div class="jobsearch-JobDescriptionSection-sectionItemKey icl-u-textBold">
               Job Type
              </div>
<div>
               Full-time
              </div>
<div>
               Contract
              </div>
</div>
</div><h2 class="jobsearch-JobDescriptionSection-jobDescriptionTitle icl-u-xs-my--md" id="jobDescriptionTitle">
            Full Job Description
           </h2><div class="jobsearch-jobDescriptionText" id="jobDescriptionText">
<div>
<p>
<b>
               Data Engineer
              </b>
</p>
<p>
<b>
               Pay Scale:
              </b>
              $73 p/h,
              <b>
               W2
              </b>
              , no benefits
             </p>
<p>
<b>
               Duration:
              </b>
              Full Time/Contract
             </p>
<p>
<b>
               Status
              </b>
              :
              <b>
               US Citizen or Green Card only
              </b>
</p>
<p>
<b>
               Reports To:
              </b>
              Project Manager
             </p>
<p>
<b>
               Working Hours:
              </b>
              Normal business hours
             </p>
<p>
<b>
               Work Location:
              </b>
              Onsite, Customer Premises, Vancouver, WA 98683
             </p>
<br/>
<p>
</p>
<p>
<b>
               Summary/Objective:
              </b>
</p>
<p>
              Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location
              <b>
               .
              </b>
</p>
<br/>
<p>
</p>
<p>
              The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.
              <br/>
<br/>
<b>
               Responsibilities
              </b>
</p>
<ul>
<li>
               Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
              </li>
<li>
               Analyzes design and determines coding, programming, and integration activities required based on general objectives.
              </li>
<li>
               Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
              </li>
<li>
               Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
              </li>
<li>
               Collaborates and communicates with project team regarding project progress and issue resolution.
              </li>
<li>
               Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
              </li>
<li>
               Collaborates with peers, engineers, data scientists and project team.
              </li>
<li>
               Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
              </li>
</ul>
<p>
<b>
               What you bring :
              </b>
</p>
<ul>
<li>
               Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
              </li>
<li>
               6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
              </li>
<li>
               3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
              </li>
<li>
               3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive &amp; Delta Lake)
              </li>
<li>
               3+ years experience in Workflow orchestration tools such as Airflow etc.
              </li>
<li>
               3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
              </li>
<li>
               Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
              </li>
<li>
               Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
              </li>
<li>
               3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
              </li>
<li>
               Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
              </li>
<li>
               Strong experience in coding languages like Python, Scala &amp; Java
              </li>
</ul>
<p>
<b>
               Knowledge and Skills
              </b>
</p>
<ul>
<li>
               Fluent in relational based systems and writing complex SQL.
              </li>
<li>
               Fluent in complex, distributed and massively parallel systems.
              </li>
<li>
               Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
              </li>
<li>
               Strong understanding of database technologies and management systems.
              </li>
<li>
               Strong understanding of data structures and algorithms
              </li>
<li>
               Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
              </li>
<li>
               Strong analytical and problem-solving skills.
              </li>
</ul>
<p>
<b>
               Nice to Have
              </b>
</p>
<ul>
<li>
               Experience with transformation tools such as dbt.
              </li>
<li>
               Have experience in building realtime streaming data pipelines
              </li>
<li>
               Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc
              </li>
</ul>
<br/>
<p>
</p>
<p>
              .EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.
             </p>
<br/>
<p>
</p>
<p>
              Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.
             </p>
</div>
<p>
</p>
</div></html>