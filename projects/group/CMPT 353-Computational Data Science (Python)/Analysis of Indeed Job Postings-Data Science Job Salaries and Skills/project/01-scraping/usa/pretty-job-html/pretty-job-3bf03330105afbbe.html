<html><div class="jobsearch-JobInfoHeader-title-container">
<h1 class="icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title">
              Data Engineer
             </h1>
</div>None<div class="jobsearch-JobDescriptionSection-section" id="jobDetailsSection">
<div class="jobsearch-JobDescriptionSection-title">
<h2 class="jobsearch-JobDescriptionSection-title--main icl-u-textBold" id="jobDetails" tabindex="-1">
               Job details
              </h2>
</div>
<div class="jobsearch-JobDescriptionSection-sectionItem">
<div class="jobsearch-JobDescriptionSection-sectionItemKey icl-u-textBold">
               Salary
              </div>
<span class="icl-u-xs-mr--xs">
               $110,000 a year
              </span>
</div>
<div class="jobsearch-JobDescriptionSection-sectionItem">
<div class="jobsearch-JobDescriptionSection-sectionItemKey icl-u-textBold">
               Job Type
              </div>
<div>
               Full-time
              </div>
</div>
</div><h2 class="jobsearch-JobDescriptionSection-jobDescriptionTitle icl-u-xs-my--md" id="jobDescriptionTitle">
            Full Job Description
           </h2><div class="jobsearch-jobDescriptionText" id="jobDescriptionText">
<p>
</p>
<div>
<p>
<b>
               ABOUT US:
              </b>
</p>
<p>
              GSC is a major transportation company located in the Port of Oakland. With 30 years of industry experience, GSC Solutions is your trusted and established carrier, dedicated to delivering with certainty!
             </p>
<p>
<b>
               ABOUT THE ROLE:
              </b>
</p>
<p>
              GSC is looking for a Data Engineer to be a technical powerhouse to help us scale our data infrastructure, dashboards, and tools to meet growing business needs.
             </p>
<p>
              The following reflects management’s definition of essential functions for this job but does not restrict the tasks that may be assigned. Management may assign or reassign duties and responsibilities to this job at any time due to reasonable accommodation or other reasons.
             </p>
<p>
<b>
               DUTIES AND RESPONSIBILTIES:
              </b>
</p>
<ul>
<li>
               Work with business partners and stakeholders to understand data/reporting requirements
              </li>
<li>
               Work with engineering, product teams and 3rd parties to collect required data
              </li>
<li>
               Design, develop and implement large scale, high-volume, high-performance data models and pipelines for Data Lake and Data Warehouse.
              </li>
<li>
               Develop and implement data quality checks, conduct QA, and implement monitoring routines.
              </li>
<li>
               Build and implement ETL frameworks to improve code quality and reliability
              </li>
<li>
               Build and enforce common design patterns to increase code maintainability
              </li>
<li>
               Manage reliability and scaling of portfolio of pipelines and data marts
              </li>
<li>
               Document new and existing models, solutions, and implementations
              </li>
<li>
               Mentor and coach team members to improve their designs and solutions
              </li>
</ul>
<p>
<b>
               ESSENTIAL QUALIFICATIONS:
              </b>
</p>
<ul>
<li>
               Bachelor’s degree or higher
              </li>
<li>
               Familiarity with Workato and/or Snowflake ‘s operating system
              </li>
<li>
               2 years of professional experience working with database technologies such as Snowflake, Azure Synapse, Azure SQL, AWS Redshift, Microsoft SQL Server, GCP BitQuery
              </li>
<li>
               Experience working in data engineering, business intelligence, or a similar role
              </li>
<li>
               Experience with ELT architectures, SQL, and database / data warehouse development using the ETL orchestration tools like HVR, DBT, Snowflake preferably or other equivalent tools
              </li>
<li>
               Advanced experience with Cloud PaaS design patterns with tools such as AWS, Azure, GCP
              </li>
<li>
               Expert in Database fundamentals, SQL, and distributed computing
              </li>
<li>
               Familiarity with streaming technologies such as Kafka/Flink
              </li>
<li>
               Experience working with Snowflake, Redshift, PostgreSQL and/or other DBMS platforms
              </li>
<li>
               Excellent communication skills and experience working with technical and non-technical teams
              </li>
<li>
               Knowledge of reporting tools such as Tableau, superset, and Looker
              </li>
<li>
               Comfortable working in fast paced environment, self-starter and self-organizing
              </li>
<li>
               Ability to think strategically, analyze and interpret market and consumer information
              </li>
</ul>
<p>
<br/>
</p>
<p>
</p>
<p>
              gum741LeHW
             </p>
</div>
</div></html>