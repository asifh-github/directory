reticulate::repl_python()
print('R job proportion:' + len(r)/len(data))
print('Python job proportion:' + len(python)/len(data))
print('Python job proportion:' + len(sql)/len(data))
quit
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
reticulate::repl_python()
import pandas as pd
import pingouin as pg
from scipy.stats import normaltest, probplot
import matplotlib.pyplot as plt
from statsmodels.graphics.factorplots import interaction_plot
import warnings
import seaborn as sns
from wordcloud import WordCloud
from matplotlib_venn import venn3
import numpy as np
warnings.filterwarnings('ignore')
data = pd.read_csv('../project/03-cleaning/for-anova/anova-data.csv')
data = data.query('salary_low > 10000').copy() #load data, data with salaries lower than 10000 are incorrectly labeled or unlabeled
data = data.query('role != "unknown"').copy()
data.loc[:,['is_remote','is_intern','role','seniority']] = data.loc[:,['is_remote','is_intern','role','seniority']].apply(pd.Categorical) #set dtypes
data = data[['salary_avg','is_remote','is_intern','role']]
data
data
pd.DataFrame(data.groupby(['role','is_intern']).salary_avg.count().rename('count'))
pd.DataFrame(data.groupby(['role','is_intern']).salary_avg.mean().rename('mean_salary'))
plt.clf()
means = data.groupby(['role','is_intern']).salary_avg.mean()
residuals = data.set_index(['role','is_intern']).salary_avg - means
temp=probplot(residuals,plot=plt)
plt.title('Normal QQ')
plt.tight_layout()
plt.show()
stds=data.groupby(['role','is_intern']).salary_avg.std().rename('std')
stds.round(3)
print(f'max/min={stds.max()/stds.min()}')
pg.anova(data=data,dv='salary_avg',between=['role','is_intern'],ss_type=3).set_index('Source').round(3)
ttest = pg.pairwise_tests(
data=data,
dv='salary_avg',
between=['is_intern','role'],
padjust='fdr_bh'
)
ttest.drop(['BF10','hedges','alternative','dof','T','Parametric','Paired','p-adjust'],axis=1).round(3)
fig, (ax1,ax2) = plt.subplots(2,1)
sns.pointplot(data=data,x='role',y='salary_avg',hue='is_intern',ax=ax2)
sns.pointplot(data=data,x='is_intern',y='salary_avg',hue='role',ax=ax1)
plt.tight_layout()
plt.show()
means=data.groupby(['role','is_intern']).salary_avg.mean()
means.sort_values().round(3)
df = pd.read_csv('../project/07-job-words/for-skill-analysis/data.csv').set_index('job_key')
df
counts = df.groupby('skill').skill.count().sort_values(ascending=False)
counts
plt.clf()
wc = WordCloud(background_color='white').generate_from_frequencies(counts)
plt.imshow(wc)
plt.axis('off')
plt.show()
r = df.loc[df.skill == 'R'].reset_index()
r['skill'] = 'R'
python = df.loc[df.skill.str.lower().str.contains('python')].reset_index().drop_duplicates()
python['skill'] = 'Python'
sql = df.loc[df.skill.str.lower().str.contains('sql')].reset_index().drop_duplicates()
sql['skill'] = 'SQL'
rps = pd.concat([r,python,sql])
rps['skill'] = pd.Categorical(rps.skill)
plt.clf()
sns.histplot(data=rps,x='skill')
plt.show()
print('R job proportion:' + len(r)/len(data))
print('Python job proportion:' + len(python)/len(data))
print('Python job proportion:' + len(sql)/len(data))
print(f'R job proportion:{len(r)/len(data)}')
print(f'Python job proportion:{len(python)/len(data)}')
print(f'SQL job proportion:{len(sql)/len(data)}')
print(f'R job proportion:{len(r)/len(df)}')
print(f'Python job proportion:{len(python)/len(df)}')
print(f'SQL job proportion:{len(sql)/len(df)}')
n_jobs = df.job_key.nunique()
print(f'R job proportion:{len(r)/njobs}')
print(f'Python job proportion:{len(python)/njobs}')
print(f'SQL job proportion:{len(sql)/njobs}')
n_jobs = df.job_key.nunique()
print(f'R job proportion:{len(r)/n_jobs}')
print(f'Python job proportion:{len(python)/n_jobs}')
print(f'SQL job proportion:{len(sql)/n_jobs}')
n_jobs = df.index.nunique()
print(f'R job proportion:{len(r)/n_jobs}')
print(f'Python job proportion:{len(python)/n_jobs}')
print(f'SQL job proportion:{len(sql)/n_jobs}')
n_jobs = df.index.nunique()
print(f'R job proportion:{len(r)/n_jobs}')
print(f'Python job proportion:{len(python)/n_jobs}')
print(f'SQL job proportion:{len(sql)/n_jobs}')
